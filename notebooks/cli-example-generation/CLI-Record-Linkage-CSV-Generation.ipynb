{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLI Record Linkage CSV Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boilerplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import logging\n",
    "reload(logging)\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.INFO, datefmt='%H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import tempfile\n",
    "\n",
    "dataset_url = 'https://dbs.uni-leipzig.de/file/Amazon-GoogleProducts.zip'\n",
    "tf = tempfile.NamedTemporaryFile(mode='r', delete=False)\n",
    "tf.close()\n",
    "\n",
    "urllib.request.urlretrieve(dataset_url, tf.name);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Amazon.csv', 'GoogleProducts.csv', 'Amzon_GoogleProducts_perfectMapping.csv']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "td = tempfile.TemporaryDirectory()\n",
    "\n",
    "with zipfile.ZipFile(tf.name, \"r\") as zf:\n",
    "    zf.extractall(td.name)\n",
    "\n",
    "os.listdir(td.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "from entity_embed.data_utils.utils import Enumerator\n",
    "\n",
    "id_enumerator = Enumerator()\n",
    "row_dict = {}\n",
    "source_field = '__source'\n",
    "left_source = 'amazon'\n",
    "right_source = 'google'\n",
    "\n",
    "with open(f'{td.name}/Amazon.csv', newline='', encoding=\"latin1\") as f:\n",
    "    for row in csv.DictReader(f):\n",
    "        row['id'] = id_enumerator[row[\"id\"]]\n",
    "        row['name'] = row.pop('title')  # in Amazon, name is called title\n",
    "        row[source_field] = left_source\n",
    "        row_dict[row['id']] = row\n",
    "\n",
    "with open(f'{td.name}/GoogleProducts.csv', newline='', encoding=\"latin1\") as f:\n",
    "    for row in csv.DictReader(f):\n",
    "        row['id'] = id_enumerator[row[\"id\"]]\n",
    "        row[source_field] = right_source\n",
    "        row_dict[row['id']] = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1300"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_pair_set = set()\n",
    "\n",
    "with open(f'{td.name}/Amzon_GoogleProducts_perfectMapping.csv', newline='') as f:\n",
    "    for row in csv.DictReader(f):\n",
    "        id_left = id_enumerator[row['idAmazon']]\n",
    "        id_right = id_enumerator[row['idGoogleBase']]\n",
    "        pos_pair_set.add((id_left, id_right))\n",
    "\n",
    "len(pos_pair_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3290"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from entity_embed.data_utils import utils\n",
    "\n",
    "cluster_mapping, cluster_dict = utils.id_pairs_to_cluster_mapping_and_dict(pos_pair_set, row_dict)\n",
    "len(cluster_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_field = 'cluster'\n",
    "\n",
    "utils.assign_clusters(row_dict=row_dict, cluster_field=cluster_field, cluster_mapping=cluster_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:35:55 INFO:Singleton cluster sizes (train, valid, test):(437, 437, 1311)\n",
      "13:35:55 INFO:Plural cluster sizes (train, valid, test):(221, 221, 663)\n"
     ]
    }
   ],
   "source": [
    "from entity_embed.data_utils import utils\n",
    "\n",
    "train_row_dict, valid_row_dict, test_row_dict = utils.split_row_dict_on_clusters(\n",
    "    row_dict=row_dict,\n",
    "    cluster_field=cluster_field,\n",
    "    train_proportion=0.2,\n",
    "    valid_proportion=0.2,\n",
    "    random_seed=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:35:55 INFO:Singleton cluster sizes (train, valid, test):(655, 655, 1)\n",
      "13:35:55 INFO:Plural cluster sizes (train, valid, test):(331, 331, 1)\n"
     ]
    }
   ],
   "source": [
    "from entity_embed.data_utils import utils\n",
    "\n",
    "test_row_dict, unlabeled_row_dict, rest_dict = utils.split_row_dict_on_clusters(\n",
    "    row_dict=test_row_dict,\n",
    "    cluster_field=cluster_field,\n",
    "    train_proportion=0.5,\n",
    "    valid_proportion=0.5,\n",
    "    random_seed=random_seed)\n",
    "\n",
    "unlabeled_row_dict.update(rest_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "td.cleanup()\n",
    "os.remove(tf.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_row_dicts = [\n",
    "    train_row_dict,\n",
    "    valid_row_dict,\n",
    "    test_row_dict,\n",
    "    unlabeled_row_dict\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_list = ['name', 'description', 'manufacturer', 'price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode\n",
    "import itertools\n",
    "from entity_embed import default_tokenizer\n",
    "\n",
    "def clean_str(s):\n",
    "    max_tokens = 30\n",
    "    max_chars = 1000\n",
    "    s = unidecode.unidecode(s).lower().strip()\n",
    "    s_tokens = default_tokenizer(s)[:max_tokens]\n",
    "    return ' '.join(s_tokens)[:max_chars]\n",
    "\n",
    "for row_dict_ in all_row_dicts:\n",
    "    for row in row_dict_.values():\n",
    "        for field in field_list:\n",
    "            row[field] = clean_str(row[field])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "rnd = random.Random(random_seed)\n",
    "\n",
    "fieldnames = ['id', *field_list, '__source']\n",
    "\n",
    "def write_csv(filepath, row_dict_, fieldnames, include_labels):\n",
    "    if include_labels:\n",
    "        fieldnames = [cluster_field] + fieldnames\n",
    "    \n",
    "    with open(filepath, 'w', encoding='utf-8', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in row_dict_.values():\n",
    "            writer.writerow({k: v for k, v in row.items() if k in fieldnames})\n",
    "                \n",
    "write_csv('../../example-data/rl-train.csv', train_row_dict, fieldnames, include_labels=True)\n",
    "write_csv('../../example-data/rl-valid.csv', valid_row_dict, fieldnames, include_labels=True)\n",
    "write_csv('../../example-data/rl-test.csv', test_row_dict, fieldnames, include_labels=True)\n",
    "write_csv('../../example-data/rl-unlabeled.csv', unlabeled_row_dict, fieldnames, include_labels=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "381"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled_pos_pairs = utils.cluster_dict_to_id_pairs(\n",
    "    cluster_dict=utils.row_dict_to_cluster_dict(unlabeled_row_dict, cluster_field),\n",
    "    left_id_set={row['id'] for row in row_dict.values() if row[source_field] == left_source},\n",
    "    right_id_set={row['id'] for row in row_dict.values() if row[source_field] == right_source}\n",
    ")\n",
    "len(unlabeled_pos_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('../../example-data/rl-unlabeled-pos-pairs.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(list(unlabeled_pos_pairs), f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from entity_embed.data_utils.field_config_parser import DEFAULT_ALPHABET\n",
    "\n",
    "alphabet = DEFAULT_ALPHABET\n",
    "field_config_dict = {\n",
    "    'name': {\n",
    "        'field_type': \"MULTITOKEN\",\n",
    "        'tokenizer': \"entity_embed.default_tokenizer\",\n",
    "        'alphabet': alphabet,\n",
    "        'max_str_len': None,  # compute\n",
    "    },\n",
    "    'semantic_name': {\n",
    "        'key': 'name',\n",
    "        'field_type': \"SEMANTIC_MULTITOKEN\",\n",
    "        'tokenizer': \"entity_embed.default_tokenizer\",\n",
    "        'vocab': \"fasttext.en.300d\",\n",
    "    },\n",
    "    'description': {\n",
    "        'field_type': \"MULTITOKEN\",\n",
    "        'tokenizer': \"entity_embed.default_tokenizer\",\n",
    "        'alphabet': alphabet,\n",
    "        'max_str_len': None,  # compute\n",
    "    },\n",
    "    'semantic_description': {\n",
    "        'key': 'description',\n",
    "        'field_type': \"SEMANTIC_MULTITOKEN\",\n",
    "        'tokenizer': \"entity_embed.default_tokenizer\",\n",
    "        'vocab': \"fasttext.en.300d\",\n",
    "    },\n",
    "    'manufacturer': {\n",
    "        'field_type': \"MULTITOKEN\",\n",
    "        'tokenizer': \"entity_embed.default_tokenizer\",\n",
    "        'alphabet': alphabet,\n",
    "        'max_str_len': None,  # compute\n",
    "    },\n",
    "    'price': {\n",
    "        'field_type': \"STRING\",\n",
    "        'tokenizer': \"entity_embed.default_tokenizer\",\n",
    "        'alphabet': alphabet,\n",
    "        'max_str_len': None,  # compute\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../example-data/rl-field-config.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(field_config_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "entity_embed_train \\\n",
    "    --field_config_json example-data/rl-field-config.json \\\n",
    "    --train_csv example-data/rl-train.csv \\\n",
    "    --valid_csv example-data/rl-valid.csv \\\n",
    "    --test_csv example-data/rl-test.csv \\\n",
    "    --unlabeled_csv example-data/rl-unlabeled.csv \\\n",
    "    --csv_encoding utf-8 \\\n",
    "    --cluster_field cluster \\\n",
    "    --source_field __source \\\n",
    "    --left_source amazon \\\n",
    "    --embedding_size 300 \\\n",
    "    --lr 0.001 \\\n",
    "    --min_epochs 5 \\\n",
    "    --max_epochs 100 \\\n",
    "    --early_stop_monitor valid_recall_at_0.3 \\\n",
    "    --early_stop_min_delta 0 \\\n",
    "    --early_stop_patience 20 \\\n",
    "    --early_stop_mode max \\\n",
    "    --tb_save_dir tb_logs \\\n",
    "    --tb_name rl-example \\\n",
    "    --check_val_every_n_epoch 1 \\\n",
    "    --batch_size 32 \\\n",
    "    --eval_batch_size 64 \\\n",
    "    --num_workers -1 \\\n",
    "    --multiprocessing_context fork \\\n",
    "    --sim_threshold 0.3 \\\n",
    "    --sim_threshold 0.5 \\\n",
    "    --sim_threshold 0.7 \\\n",
    "    --ann_k 100 \\\n",
    "    --m 64 \\\n",
    "    --max_m0 64 \\\n",
    "    --ef_construction 150 \\\n",
    "    --ef_search -1 \\\n",
    "    --random_seed 42 \\\n",
    "    --model_save_dir trained-models/rl/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "entity_embed_predict \\\n",
    "    --model_save_filepath \"trained-models/rl/...fill-here...\" \\\n",
    "    --field_config_json example-data/rl-example-field-config.json \\\n",
    "    --ann_k 100 \\\n",
    "    --ef_search -1 \\\n",
    "    --ef_construction 150 \\\n",
    "    --max_m0 64 \\\n",
    "    --m 64 \\\n",
    "    --multiprocessing_context fork \\\n",
    "    --num_workers -1 \\\n",
    "    --sim_threshold 0.3 \\\n",
    "    --random_seed 42 \\\n",
    "    --eval_batch_size 50 \\\n",
    "    --csv_encoding utf-8 \\\n",
    "    --unlabeled_csv example-data/rl-unlabeled.csv \\\n",
    "    --output_json example-data/rl-prediction.json\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
