{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLI Record Linkage CSV Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boilerplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import logging\n",
    "reload(logging)\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.INFO, datefmt='%H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, '../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import tempfile\n",
    "\n",
    "dataset_url = 'https://dbs.uni-leipzig.de/file/Amazon-GoogleProducts.zip'\n",
    "tf = tempfile.NamedTemporaryFile(mode='r', delete=False)\n",
    "tf.close()\n",
    "\n",
    "urllib.request.urlretrieve(dataset_url, tf.name);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Amazon.csv', 'GoogleProducts.csv', 'Amzon_GoogleProducts_perfectMapping.csv']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "td = tempfile.TemporaryDirectory()\n",
    "\n",
    "with zipfile.ZipFile(tf.name, \"r\") as zf:\n",
    "    zf.extractall(td.name)\n",
    "\n",
    "os.listdir(td.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "from entity_embed.data_utils.utils import Enumerator\n",
    "\n",
    "id_enumerator = Enumerator()\n",
    "record_dict = {}\n",
    "source_field = '__source'\n",
    "left_source = 'amazon'\n",
    "right_source = 'google'\n",
    "\n",
    "with open(f'{td.name}/Amazon.csv', newline='', encoding=\"latin1\") as f:\n",
    "    for record in csv.DictReader(f):\n",
    "        record['id'] = id_enumerator[record[\"id\"]]\n",
    "        record[source_field] = left_source\n",
    "        del record['description']  # drop description, for benchmarking\n",
    "        record_dict[record['id']] = record\n",
    "\n",
    "with open(f'{td.name}/GoogleProducts.csv', newline='', encoding=\"latin1\") as f:\n",
    "    for record in csv.DictReader(f):\n",
    "        record['id'] = id_enumerator[record[\"id\"]]\n",
    "        record['title'] = record.pop('name')  # in Google, title is called name\n",
    "        record[source_field] = right_source\n",
    "        del record['description']  # drop description, for benchmarking\n",
    "        record_dict[record['id']] = record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1300"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_pair_set = set()\n",
    "\n",
    "with open(f'{td.name}/Amzon_GoogleProducts_perfectMapping.csv', newline='') as f:\n",
    "    for record in csv.DictReader(f):\n",
    "        id_left = id_enumerator[record['idAmazon']]\n",
    "        id_right = id_enumerator[record['idGoogleBase']]\n",
    "        pos_pair_set.add((id_left, id_right))\n",
    "\n",
    "len(pos_pair_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3290"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from entity_embed.data_utils import utils\n",
    "\n",
    "cluster_mapping, cluster_dict = utils.id_pairs_to_cluster_mapping_and_dict(pos_pair_set, record_dict)\n",
    "len(cluster_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_field = 'cluster'\n",
    "\n",
    "utils.assign_clusters(record_dict=record_dict, cluster_field=cluster_field, cluster_mapping=cluster_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12:43:09 INFO:Singleton cluster sizes (train, valid, test):(1092, 437, 656)\n",
      "12:43:09 INFO:Plural cluster sizes (train, valid, test):(552, 221, 332)\n"
     ]
    }
   ],
   "source": [
    "from entity_embed.data_utils import utils\n",
    "\n",
    "train_record_dict, valid_record_dict, test_record_dict = utils.split_record_dict_on_clusters(\n",
    "    record_dict=record_dict,\n",
    "    cluster_field=cluster_field,\n",
    "    train_proportion=0.5,\n",
    "    valid_proportion=0.2,\n",
    "    random_seed=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12:43:09 INFO:Singleton cluster sizes (train, valid, test):(328, 328, 0)\n",
      "12:43:09 INFO:Plural cluster sizes (train, valid, test):(166, 166, 0)\n"
     ]
    }
   ],
   "source": [
    "from entity_embed.data_utils import utils\n",
    "\n",
    "test_record_dict, unlabeled_record_dict, rest_dict = utils.split_record_dict_on_clusters(\n",
    "    record_dict=test_record_dict,\n",
    "    cluster_field=cluster_field,\n",
    "    train_proportion=0.5,\n",
    "    valid_proportion=0.5,\n",
    "    random_seed=random_seed)\n",
    "\n",
    "unlabeled_record_dict.update(rest_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "del record_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "td.cleanup()\n",
    "os.remove(tf.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_record_dicts = [\n",
    "    train_record_dict,\n",
    "    valid_record_dict,\n",
    "    test_record_dict,\n",
    "    unlabeled_record_dict\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_list = ['title', 'manufacturer', 'price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode\n",
    "\n",
    "def clean_str(s):\n",
    "    return unidecode.unidecode(s).lower().strip()\n",
    "\n",
    "for record_dict_ in all_record_dicts:\n",
    "    for record in record_dict_.values():\n",
    "        for field in field_list:\n",
    "            record[field] = clean_str(record[field])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "rnd = random.Random(random_seed)\n",
    "\n",
    "fieldnames = ['id', *field_list, '__source']\n",
    "\n",
    "def write_csv(filepath, record_dict_, fieldnames, include_labels):\n",
    "    if include_labels:\n",
    "        fieldnames = [cluster_field] + fieldnames\n",
    "    \n",
    "    with open(filepath, 'w', encoding='utf-8', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for record in record_dict_.values():\n",
    "            writer.writerow({k: v for k, v in record.items() if k in fieldnames})\n",
    "                \n",
    "write_csv('../../example-data/rl-train.csv', train_record_dict, fieldnames, include_labels=True)\n",
    "write_csv('../../example-data/rl-valid.csv', valid_record_dict, fieldnames, include_labels=True)\n",
    "write_csv('../../example-data/rl-test.csv', test_record_dict, fieldnames, include_labels=True)\n",
    "write_csv('../../example-data/rl-unlabeled.csv', unlabeled_record_dict, fieldnames, include_labels=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled_pos_pairs = utils.cluster_dict_to_id_pairs(\n",
    "    cluster_dict=utils.record_dict_to_cluster_dict(unlabeled_record_dict, cluster_field),\n",
    "    left_id_set={record['id'] for record in unlabeled_record_dict.values() if record[source_field] == left_source},\n",
    "    right_id_set={record['id'] for record in unlabeled_record_dict.values() if record[source_field] == right_source}\n",
    ")\n",
    "len(unlabeled_pos_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('../../example-data/rl-unlabeled-pos-pairs.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(list(unlabeled_pos_pairs), f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from entity_embed.data_utils.field_config_parser import DEFAULT_ALPHABET\n",
    "\n",
    "alphabet = DEFAULT_ALPHABET\n",
    "field_config_dict = {\n",
    "    'title': {\n",
    "        'field_type': \"MULTITOKEN\",\n",
    "        'tokenizer': \"entity_embed.default_tokenizer\",\n",
    "        'alphabet': alphabet,\n",
    "        'max_str_len': None,  # compute\n",
    "    },\n",
    "    'title_semantic': {\n",
    "        'key': 'title',\n",
    "        'field_type': \"SEMANTIC\",\n",
    "        'tokenizer': \"entity_embed.default_tokenizer\",\n",
    "        'vocab': \"fasttext.en.300d\",\n",
    "    },\n",
    "    'manufacturer': {\n",
    "        'field_type': \"MULTITOKEN\",\n",
    "        'tokenizer': \"entity_embed.default_tokenizer\",\n",
    "        'alphabet': alphabet,\n",
    "        'max_str_len': None,  # compute\n",
    "    },\n",
    "    'price': {\n",
    "        'field_type': \"STRING\",\n",
    "        'tokenizer': \"entity_embed.default_tokenizer\",\n",
    "        'alphabet': alphabet,\n",
    "        'max_str_len': None,  # compute\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../example-data/rl-field-config.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(field_config_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "entity_embed_train \\\n",
    "    --field_config_json example-data/rl-field-config.json \\\n",
    "    --train_csv example-data/rl-train.csv \\\n",
    "    --valid_csv example-data/rl-valid.csv \\\n",
    "    --test_csv example-data/rl-test.csv \\\n",
    "    --unlabeled_csv example-data/rl-unlabeled.csv \\\n",
    "    --csv_encoding utf-8 \\\n",
    "    --cluster_field cluster \\\n",
    "    --source_field __source \\\n",
    "    --left_source amazon \\\n",
    "    --embedding_size 300 \\\n",
    "    --lr 0.001 \\\n",
    "    --min_epochs 5 \\\n",
    "    --max_epochs 100 \\\n",
    "    --early_stop_monitor valid_recall_at_0.3 \\\n",
    "    --early_stop_min_delta 0 \\\n",
    "    --early_stop_patience 20 \\\n",
    "    --early_stop_mode max \\\n",
    "    --tb_save_dir tb_logs \\\n",
    "    --tb_name rl-example \\\n",
    "    --check_val_every_n_epoch 1 \\\n",
    "    --batch_size 32 \\\n",
    "    --eval_batch_size 64 \\\n",
    "    --num_workers -1 \\\n",
    "    --multiprocessing_context fork \\\n",
    "    --sim_threshold 0.3 \\\n",
    "    --sim_threshold 0.5 \\\n",
    "    --sim_threshold 0.7 \\\n",
    "    --ann_k 100 \\\n",
    "    --m 64 \\\n",
    "    --max_m0 64 \\\n",
    "    --ef_construction 150 \\\n",
    "    --ef_search -1 \\\n",
    "    --random_seed 42 \\\n",
    "    --model_save_dir trained-models/rl/ \\\n",
    "    --use_gpu 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "entity_embed_predict \\\n",
    "    --model_save_filepath \"trained-models/rl/...fill-here...\" \\\n",
    "    --unlabeled_csv example-data/rl-unlabeled.csv \\\n",
    "    --csv_encoding utf-8 \\\n",
    "    --source_field __source \\\n",
    "    --left_source amazon \\\n",
    "    --eval_batch_size 50 \\\n",
    "    --num_workers -1 \\\n",
    "    --multiprocessing_context fork \\\n",
    "    --sim_threshold 0.3 \\\n",
    "    --ann_k 100 \\\n",
    "    --m 64 \\\n",
    "    --max_m0 64 \\\n",
    "    --ef_construction 150 \\\n",
    "    --ef_search -1 \\\n",
    "    --random_seed 42 \\\n",
    "    --output_json example-data/rl-prediction.json \\\n",
    "    --use_gpu 1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision 0.14876690533015116\n",
      "recall 1.0\n",
      "f1 0.25900277008310246\n",
      "pe_ratio 1.8458149779735682\n"
     ]
    }
   ],
   "source": [
    "from entity_embed.evaluation import evaluate_output_json\n",
    "\n",
    "precision, recall, f1, pe_ratio = evaluate_output_json(\n",
    "    unlabeled_csv_filepath='../../example-data/rl-unlabeled.csv',\n",
    "    output_json_filepath='../../example-data/rl-prediction.json',\n",
    "    pos_pair_json_filepath='../../example-data/rl-unlabeled-pos-pairs.json'\n",
    ")\n",
    "print(\"precision\", precision)\n",
    "print(\"recall\", recall)\n",
    "print(\"f1\", f1)\n",
    "print(\"pe_ratio\", pe_ratio) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
