{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLI Deduplication CSV Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boilerplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import logging\n",
    "reload(logging)\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.INFO, datefmt='%H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, '../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import tempfile\n",
    "\n",
    "dataset_url = 'https://www.informatik.uni-leipzig.de/~saeedi/musicbrainz-20-A01.csv.dapo'\n",
    "tf = tempfile.NamedTemporaryFile(mode='r', delete=False)\n",
    "tf.close()\n",
    "\n",
    "urllib.request.urlretrieve(dataset_url, tf.name);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "record_dict = {}\n",
    "\n",
    "with open(tf.name, newline=\"\") as f:\n",
    "    for current_record_id, record in enumerate(csv.DictReader(f)):\n",
    "        record[\"id\"] = current_record_id\n",
    "        record[\"cluster\"] = int(record.pop(\"CID\"))  # rename CID to \"cluster\" and convert to int\n",
    "        record_dict[record[\"id\"]] = record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from entity_embed.data_utils import utils\n",
    "\n",
    "cluster_field = 'cluster'\n",
    "cluster_dict = utils.record_dict_to_cluster_dict(record_dict, cluster_field)\n",
    "cluster_mapping = {\n",
    "    id_: cluster_id for cluster_id, cluster in cluster_dict.items() for id_ in cluster\n",
    "}\n",
    "len(cluster_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12:30:01 INFO:Singleton cluster sizes (train, valid, test):(1000, 1000, 3000)\n",
      "12:30:01 INFO:Plural cluster sizes (train, valid, test):(1000, 1000, 3000)\n"
     ]
    }
   ],
   "source": [
    "from entity_embed.data_utils import utils\n",
    "\n",
    "train_record_dict, valid_record_dict, test_record_dict = utils.split_record_dict_on_clusters(\n",
    "    record_dict=record_dict,\n",
    "    cluster_field=cluster_field,\n",
    "    train_proportion=0.2,\n",
    "    valid_proportion=0.2,\n",
    "    random_seed=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12:30:01 INFO:Singleton cluster sizes (train, valid, test):(1500, 1500, 0)\n",
      "12:30:01 INFO:Plural cluster sizes (train, valid, test):(1500, 1500, 0)\n"
     ]
    }
   ],
   "source": [
    "from entity_embed.data_utils import utils\n",
    "\n",
    "test_record_dict, unlabeled_record_dict, rest_dict = utils.split_record_dict_on_clusters(\n",
    "    record_dict=test_record_dict,\n",
    "    cluster_field=cluster_field,\n",
    "    train_proportion=0.5,\n",
    "    valid_proportion=0.5,\n",
    "    random_seed=random_seed)\n",
    "\n",
    "unlabeled_record_dict.update(rest_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "del record_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "tf.close()\n",
    "os.remove(tf.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_record_dicts = [\n",
    "    train_record_dict,\n",
    "    valid_record_dict,\n",
    "    test_record_dict,\n",
    "    unlabeled_record_dict\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_list = ['number', 'title', 'artist', 'album', 'year', 'language']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode\n",
    "\n",
    "def clean_str(s):\n",
    "    return unidecode.unidecode(s).lower().strip()\n",
    "\n",
    "for record_dict_ in all_record_dicts:\n",
    "    for record in record_dict_.values():\n",
    "        for field in field_list:\n",
    "            record[field] = clean_str(record[field])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "rnd = random.Random(random_seed)\n",
    "\n",
    "fieldnames = ['id', *field_list]\n",
    "\n",
    "def write_csv(filepath, record_dict_, fieldnames, include_labels):\n",
    "    if include_labels:\n",
    "        fieldnames = [cluster_field] + fieldnames\n",
    "    \n",
    "    with open(filepath, 'w', encoding='utf-8', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for record in record_dict_.values():\n",
    "            writer.writerow({k: v for k, v in record.items() if k in fieldnames})\n",
    "                \n",
    "write_csv('../../example-data/er-train.csv', train_record_dict, fieldnames, include_labels=True)\n",
    "write_csv('../../example-data/er-valid.csv', valid_record_dict, fieldnames, include_labels=True)\n",
    "write_csv('../../example-data/er-test.csv', test_record_dict, fieldnames, include_labels=True)\n",
    "write_csv('../../example-data/er-unlabeled.csv', unlabeled_record_dict, fieldnames, include_labels=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4932"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled_pos_pairs = utils.cluster_dict_to_id_pairs(\n",
    "    cluster_dict=utils.record_dict_to_cluster_dict(unlabeled_record_dict, cluster_field),\n",
    ")\n",
    "len(unlabeled_pos_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('../../example-data/er-unlabeled-pos-pairs.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(list(unlabeled_pos_pairs), f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from entity_embed.data_utils.field_config_parser import DEFAULT_ALPHABET\n",
    "\n",
    "alphabet = DEFAULT_ALPHABET\n",
    "field_config_dict = {\n",
    "    'number': {\n",
    "        'field_type': \"STRING\",\n",
    "        'alphabet': alphabet,\n",
    "        'max_str_len': None,  # compute\n",
    "    },\n",
    "    'title': {\n",
    "        'field_type': \"MULTITOKEN\",\n",
    "        'tokenizer': \"entity_embed.default_tokenizer\",\n",
    "        'alphabet': alphabet,\n",
    "        'max_str_len': None,  # compute\n",
    "    },\n",
    "    'title_semantic': {\n",
    "        'key': 'title',\n",
    "        'field_type': \"SEMANTIC_MULTITOKEN\",\n",
    "        'tokenizer': \"entity_embed.default_tokenizer\",\n",
    "        'vocab': \"fasttext.en.300d\",\n",
    "    },\n",
    "    'artist': {\n",
    "        'field_type': \"MULTITOKEN\",\n",
    "        'tokenizer': \"entity_embed.default_tokenizer\",\n",
    "        'alphabet': alphabet,\n",
    "        'max_str_len': None,  # compute\n",
    "    },\n",
    "    'album': {\n",
    "        'field_type': \"MULTITOKEN\",\n",
    "        'tokenizer': \"entity_embed.default_tokenizer\",\n",
    "        'alphabet': alphabet,\n",
    "        'max_str_len': None,  # compute\n",
    "    },\n",
    "    'album_semantic': {\n",
    "        'key': 'album',\n",
    "        'field_type': \"SEMANTIC_MULTITOKEN\",\n",
    "        'tokenizer': \"entity_embed.default_tokenizer\",\n",
    "        'vocab': \"fasttext.en.300d\",\n",
    "    },\n",
    "    'year': {\n",
    "        'field_type': \"STRING\",\n",
    "        'alphabet': alphabet,\n",
    "        'max_str_len': None,  # compute\n",
    "    },\n",
    "    'language': {\n",
    "        'field_type': \"STRING\",\n",
    "        'alphabet': alphabet,\n",
    "        'max_str_len': None,  # compute\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../example-data/er-field-config.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(field_config_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "entity_embed_train \\\n",
    "    --field_config_json example-data/er-field-config.json \\\n",
    "    --train_csv example-data/er-train.csv \\\n",
    "    --valid_csv example-data/er-valid.csv \\\n",
    "    --test_csv example-data/er-test.csv \\\n",
    "    --unlabeled_csv example-data/er-unlabeled.csv \\\n",
    "    --csv_encoding utf-8 \\\n",
    "    --cluster_field cluster \\\n",
    "    --embedding_size 300 \\\n",
    "    --lr 0.001 \\\n",
    "    --min_epochs 5 \\\n",
    "    --max_epochs 100 \\\n",
    "    --early_stop_monitor valid_recall_at_0.3 \\\n",
    "    --early_stop_min_delta 0 \\\n",
    "    --early_stop_patience 20 \\\n",
    "    --early_stop_mode max \\\n",
    "    --tb_save_dir tb_logs \\\n",
    "    --tb_name er-example \\\n",
    "    --check_val_every_n_epoch 1 \\\n",
    "    --batch_size 32 \\\n",
    "    --eval_batch_size 64 \\\n",
    "    --num_workers -1 \\\n",
    "    --multiprocessing_context fork \\\n",
    "    --sim_threshold 0.3 \\\n",
    "    --sim_threshold 0.5 \\\n",
    "    --sim_threshold 0.7 \\\n",
    "    --ann_k 100 \\\n",
    "    --m 64 \\\n",
    "    --max_m0 64 \\\n",
    "    --ef_construction 150 \\\n",
    "    --ef_search -1 \\\n",
    "    --random_seed 42 \\\n",
    "    --model_save_dir trained-models/er/ \\\n",
    "    --use_gpu 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "entity_embed_predict \\\n",
    "    --model_save_filepath \"trained-models/er/...fill-here...\" \\\n",
    "    --unlabeled_csv example-data/er-unlabeled.csv \\\n",
    "    --csv_encoding utf-8 \\\n",
    "    --eval_batch_size 50 \\\n",
    "    --num_workers -1 \\\n",
    "    --multiprocessing_context fork \\\n",
    "    --sim_threshold 0.3 \\\n",
    "    --ann_k 100 \\\n",
    "    --m 64 \\\n",
    "    --max_m0 64 \\\n",
    "    --ef_construction 150 \\\n",
    "    --ef_search -1 \\\n",
    "    --random_seed 42 \\\n",
    "    --output_json example-data/er-prediction.json \\\n",
    "    --use_gpu 1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision 0.027437097685239557\n",
      "recall 0.994728304947283\n",
      "f1 0.05340125502745712\n",
      "pe_ratio 30.638965044551064\n"
     ]
    }
   ],
   "source": [
    "from entity_embed.evaluation import evaluate_output_json\n",
    "\n",
    "precision, recall, f1, pe_ratio = evaluate_output_json(\n",
    "    unlabeled_csv_filepath='../../example-data/er-unlabeled.csv',\n",
    "    output_json_filepath='../../example-data/er-prediction.json',\n",
    "    pos_pair_json_filepath='../../example-data/er-unlabeled-pos-pairs.json'\n",
    ")\n",
    "print(\"precision\", precision)\n",
    "print(\"recall\", recall)\n",
    "print(\"f1\", f1)\n",
    "print(\"pe_ratio\", pe_ratio) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
