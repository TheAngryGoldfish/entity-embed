{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Record Linkage Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boilerplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install entity-embed\n",
    "!pip install \"matplotlib==3.1.1\" \\\n",
    "             \"pynndescent==0.5.2\" \\\n",
    "             \"scikit-learn==0.24.1\" \\\n",
    "             \"seaborn==0.11.1\" \\\n",
    "             \"unidecode==1.1.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import logging\n",
    "reload(logging)\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.INFO, datefmt='%H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import entity_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "random_seed = 42\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the [Amazon-GoogleProducts](https://dbs.uni-leipzig.de/research/projects/object_matching/benchmark_datasets_for_entity_resolution). The dataset is stored in a ZIP file with 3 CSV files: Two are the entity source files, the third one is the perfect mapping.\n",
    "\n",
    "Let's download the ZIP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import tempfile\n",
    "\n",
    "dataset_url = 'https://dbs.uni-leipzig.de/file/Amazon-GoogleProducts.zip'\n",
    "tf = tempfile.NamedTemporaryFile(mode='r', delete=False)\n",
    "tf.close()\n",
    "\n",
    "urllib.request.urlretrieve(dataset_url, tf.name);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "td = tempfile.TemporaryDirectory()\n",
    "\n",
    "with zipfile.ZipFile(tf.name, \"r\") as zf:\n",
    "    zf.extractall(td.name)\n",
    "\n",
    "os.listdir(td.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we must read the **two** CSV files `'Amazon.csv'` and `'GoogleProducts.csv'` into a **single** `dict` called `record_dict`. Therefore, `record_dict` will contain all records from the dataset, i.e., all records from both files.\n",
    "\n",
    "We'll dynamically attribute an `id` to each record using `enumerate`. Entity Embed needs that.\n",
    "\n",
    "Also, to keep track of what's source file of a record, we must keep a `__source`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "from entity_embed.data_utils.utils import Enumerator\n",
    "\n",
    "id_enumerator = Enumerator()\n",
    "record_dict = {}\n",
    "source_field = '__source'\n",
    "left_source = 'amazon'\n",
    "right_source = 'google'\n",
    "\n",
    "with open(f'{td.name}/Amazon.csv', newline='', encoding=\"latin1\") as f:\n",
    "    for record in csv.DictReader(f):\n",
    "        record['id'] = id_enumerator[record[\"id\"]]\n",
    "        record[source_field] = left_source\n",
    "        del record['description']  # drop description, for benchmarking\n",
    "        record_dict[record['id']] = record\n",
    "\n",
    "with open(f'{td.name}/GoogleProducts.csv', newline='', encoding=\"latin1\") as f:\n",
    "    for record in csv.DictReader(f):\n",
    "        record['id'] = id_enumerator[record[\"id\"]]\n",
    "        record['title'] = record.pop('name')  # in Google, title is called name\n",
    "        record[source_field] = right_source\n",
    "        del record['description']  # drop description, for benchmarking\n",
    "        record_dict[record['id']] = record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get the `pos_pair_set`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_pair_set = set()\n",
    "\n",
    "with open(f'{td.name}/Amzon_GoogleProducts_perfectMapping.csv', newline='') as f:\n",
    "    for row in csv.DictReader(f):\n",
    "        id_left = id_enumerator[row['idAmazon']]\n",
    "        id_right = id_enumerator[row['idGoogleBase']]\n",
    "        pos_pair_set.add((id_left, id_right))\n",
    "\n",
    "len(pos_pair_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note internally Entity Embed will use **clusters, not pairs**.\n",
    "\n",
    "For this dataset, the number of clusters is smaller than the number of pairs, because some clusters have more than 2 records. For example, a cluster with 3 records means 2 records from a file link to 1 record from the other file. This is not a problem for Entity Embed. In fact, it's important for Entity Embed to know the full clusters in order to learn better embeddings. See an example here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from entity_embed.data_utils import utils\n",
    "\n",
    "cluster_mapping, cluster_dict = utils.id_pairs_to_cluster_mapping_and_dict(pos_pair_set, record_dict)\n",
    "len(cluster_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_dict[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[cluster_mapping[id_] for id_ in cluster_dict[4]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use clusters, we must assign them to each record on a `cluster_field` key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_field = 'cluster'\n",
    "utils.assign_clusters(record_dict, cluster_field, cluster_mapping)\n",
    "\n",
    "for id_ in cluster_dict[4]:\n",
    "    display(record_dict[id_])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are additional pairs in clusters when compared to `pos_pair_set`. That's because it's useful for Entity Embed to also learn that cases like `(2485, 2488)` represent the same entity, even though they belong to the same dataset and won't be returned in the final pairwise results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from entity_embed.data_utils.utils import cluster_dict_to_id_pairs\n",
    "\n",
    "len(cluster_dict_to_id_pairs(cluster_dict) - pos_pair_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From all clusters, we'll use only 50% for training, and other 20% for validation, and the remaining 30% for test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from entity_embed.data_utils import utils\n",
    "\n",
    "train_record_dict, valid_record_dict, test_record_dict = utils.split_record_dict_on_clusters(\n",
    "    record_dict=record_dict,\n",
    "    cluster_field=cluster_field,\n",
    "    train_proportion=0.5,\n",
    "    valid_proportion=0.2,\n",
    "    random_seed=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note we're splitting the data on **clusters**, not records, so the record counts vary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_record_dict), len(valid_record_dict), len(test_record_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up the temporary files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "td.cleanup()\n",
    "os.remove(tf.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll perform a very minimal preprocessing of the dataset. We want to simply force ASCII chars, lowercase all chars, and strip leading and trailing whitespace.\n",
    "\n",
    "The fields we'll clean are the ones we'll use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_list = ['title', 'manufacturer', 'price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode\n",
    "\n",
    "def clean_str(s):\n",
    "    return unidecode.unidecode(s).lower().strip()\n",
    "\n",
    "for record in record_dict.values():\n",
    "    for field in field_list:\n",
    "        record[field] = clean_str(record[field])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forcing ASCII chars in this dataset is useful to improve recall because there's little difference between accented and not-accented chars here. Also, this dataset contains mostly latin chars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Entity Embed fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will define how record fields will be numericalized and encoded by the neural network. First we set an `alphabet`, here we'll use ASCII numbers, letters, symbols and space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from entity_embed.data_utils.field_config_parser import DEFAULT_ALPHABET\n",
    "\n",
    "alphabet = DEFAULT_ALPHABET\n",
    "''.join(alphabet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's worth noting you can use any alphabet you need, so the accent removal we performed is optional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we set an `field_config_dict`. It defines `field_type`s that determine how fields are processed in the neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_config_dict = {\n",
    "    'title': {\n",
    "        'field_type': \"MULTITOKEN\",\n",
    "        'tokenizer': \"entity_embed.default_tokenizer\",\n",
    "        'alphabet': alphabet,\n",
    "        'max_str_len': None,  # compute\n",
    "    },\n",
    "    'title_semantic': {\n",
    "        'key': 'title',\n",
    "        'field_type': \"SEMANTIC_MULTITOKEN\",\n",
    "        'tokenizer': \"entity_embed.default_tokenizer\",\n",
    "        'vocab': \"fasttext.en.300d\",\n",
    "    },\n",
    "    'manufacturer': {\n",
    "        'field_type': \"MULTITOKEN\",\n",
    "        'tokenizer': \"entity_embed.default_tokenizer\",\n",
    "        'alphabet': alphabet,\n",
    "        'max_str_len': None,  # compute\n",
    "    },\n",
    "    'price': {\n",
    "        'field_type': \"STRING\",\n",
    "        'tokenizer': \"entity_embed.default_tokenizer\",\n",
    "        'alphabet': alphabet,\n",
    "        'max_str_len': None,  # compute\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use our `field_config_dict` to get a `record_numericalizer`. This object will convert the strings from our records into tensors for the neural network.\n",
    "\n",
    "The same `record_numericalizer` must be used on ALL data: train, valid, test. This ensures numericalization will be consistent. Therefore, we pass `record_list=record_dict.values()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from entity_embed import FieldConfigDictParser\n",
    "\n",
    "record_numericalizer = FieldConfigDictParser.from_dict(field_config_dict, record_list=record_dict.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Data Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood, Entity Embed uses [pytorch-lightning](https://pytorch-lightning.readthedocs.io/en/latest/), so we need to create a datamodule object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from entity_embed import LinkageDataModule\n",
    "\n",
    "batch_size = 32\n",
    "eval_batch_size = 64\n",
    "datamodule = LinkageDataModule(\n",
    "    train_record_dict=train_record_dict,\n",
    "    valid_record_dict=valid_record_dict,\n",
    "    test_record_dict=test_record_dict,\n",
    "    source_field=source_field,\n",
    "    left_source=left_source,\n",
    "    cluster_field=cluster_field,\n",
    "    record_numericalizer=record_numericalizer,\n",
    "    batch_size=batch_size,\n",
    "    eval_batch_size=eval_batch_size,\n",
    "    random_seed=random_seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've used `LinkageDataModule` because we're doing Record Linkage of a multiple datasets/tables.\n",
    "\n",
    "We're NOT doing Deduplication of a single dataset here. Check the other notebook [Deduplication-Example](./Deduplication-Example.ipynb) if you want to learn how to do it with Entity Embed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the training process! Thanks to pytorch-lightning, it's easy to train, validate, and test with the same datamodule.\n",
    "\n",
    "We must choose the K of the Approximate Nearest Neighbors, i.e., the top K neighbors our model will use to find duplicates in the embedding space. Below we're setting it on `ann_k` and initializing the `LinkageEmbed` model object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from entity_embed import LinkageEmbed\n",
    "\n",
    "ann_k = 100\n",
    "model = LinkageEmbed(\n",
    "    record_numericalizer,\n",
    "    ann_k=ann_k,\n",
    "    source_field=source_field,\n",
    "    left_source=left_source,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train, Entity Embed uses [pytorch-lightning Trainer](https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html) on it's `LinkageEmbed.fit` method.\n",
    "\n",
    "Since Entity Embed is focused in recall, we'll use `valid_recall_at_0.3` for early stopping. But we'll set `min_epochs = 5` to avoid a very low precision.\n",
    "\n",
    "`0.3` here is the threshold for **cosine similarity of embedding vectors**, so possible values are between -1 and 1. We're using a validation metric, and the training process will run validation on every epoch end due to `check_val_every_n_epoch=1`.\n",
    "\n",
    "We also set `tb_name` and `tb_save_dir` to use Tensorboard. Run `tensorboard --logdir notebooks/tb_logs` to check the train and valid metrics during and after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainer = model.fit(\n",
    "    datamodule,\n",
    "    min_epochs=5,\n",
    "    max_epochs=100,\n",
    "    check_val_every_n_epoch=1,\n",
    "    early_stop_monitor=\"valid_recall_at_0.3\",\n",
    "    tb_save_dir='tb_logs',\n",
    "    tb_name='amazon-google',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LinkageEmbed.fit` keeps only the weights of the best validation model. With them, we can check the best performance on validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.validate(datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can check which fields are most important for the final embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_pool_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again with the best validation model, we can check the performance on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.test(datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entity Embed achieves Recall of ~0.99 with Pair-Entity ratio below 100 on a variety of datasets. **Entity Embed aims for high recall at the expense of precision. Therefore, this library is suited for the Blocking/Indexing stage of an Entity Resolution pipeline.**  A scalabale and noise-tolerant Blocking procedure is often the main bottleneck for performance and quality on Entity Resolution pipelines, so this library aims to solve that. Note the ANN search on embedded records returns several candidate pairs that must be filtered to find the best matching pairs, possibly with a pairwise classifier. See [Matching](#Matching) section below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-sne visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize a small sample of the test embeddings and see if they look properly clustered. First, get the embedding vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_left_vector_dict, test_right_vector_dict = model.predict(\n",
    "    record_dict=test_record_dict,\n",
    "    batch_size=eval_batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, produce the visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_sample_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vector_dict = {**test_left_vector_dict, **test_right_vector_dict}\n",
    "test_cluster_dict = utils.record_dict_to_cluster_dict(test_record_dict, cluster_field)\n",
    "vis_cluster_dict = dict(sorted(test_cluster_dict.items(), key=lambda x: len(x[1]), reverse=True)[:vis_sample_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_x = np.stack([test_vector_dict[id_] for cluster in vis_cluster_dict.values() for id_ in cluster])\n",
    "vis_y = np.array([cluster_id for cluster_id, cluster in vis_cluster_dict.items() for __ in cluster])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tnse = TSNE(metric='cosine', perplexity=20, square_distances=True, random_state=random_seed)\n",
    "tsne_results = tnse.fit_transform(vis_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "ax = sns.scatterplot(\n",
    "    x=tsne_results[:,0],\n",
    "    y=tsne_results[:,1],\n",
    "    hue=vis_y,\n",
    "    palette=sns.color_palette(\"hls\", len(vis_cluster_dict.keys())),\n",
    "    legend=\"full\",\n",
    "    alpha=0.8\n",
    ")\n",
    "for id_, (x, y) in zip(itertools.chain.from_iterable(vis_cluster_dict.values()), tsne_results):\n",
    "    # text = id_\n",
    "    text = test_record_dict[id_]['title'][:30]\n",
    "    ax.text(x + 2, y + 2, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing manually (like a production run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running in production, you only have access to the trained `model` object and the production `record_dict` (without the `cluster_field` filled, of course).\n",
    "\n",
    "So let's simulate that by removing `cluster_field` from the `test_record_dict`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "prod_test_record_dict = copy.deepcopy(test_record_dict)\n",
    "\n",
    "for record in prod_test_record_dict.values():\n",
    "    del record[cluster_field]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then call `predict_pairs` with some `ann_k` and `sim_threshold`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_threshold = 0.3\n",
    "\n",
    "found_pair_set = model.predict_pairs(\n",
    "    record_dict=prod_test_record_dict,\n",
    "    batch_size=eval_batch_size,\n",
    "    ann_k=ann_k,\n",
    "    sim_threshold=sim_threshold\n",
    ")\n",
    "len(found_pair_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check now the metrics of the found duplicate pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from entity_embed.evaluation import pair_entity_ratio\n",
    "\n",
    "pair_entity_ratio(len(found_pair_set), len(prod_test_record_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from entity_embed.evaluation import precision_and_recall\n",
    "\n",
    "precision_and_recall(found_pair_set, datamodule.test_pos_pair_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same numbers of the `trainer.test`, so our manual testing is fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can check the false positives and negatives to see if they're really difficult:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positives = list(found_pair_set - datamodule.test_pos_pair_set)\n",
    "len(false_positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_negatives = list(datamodule.test_pos_pair_set - found_pair_set)\n",
    "len(false_negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_similarity = lambda a, b: np.dot(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (id_left, id_right) in false_positives[:3]:\n",
    "    display(\n",
    "        (\n",
    "            cos_similarity(test_left_vector_dict[id_left], test_right_vector_dict[id_right]),\n",
    "            utils.subdict(record_dict[id_left], field_list), utils.subdict(record_dict[id_right], field_list)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for (id_left, id_right) in false_negatives[:3]:\n",
    "    display(\n",
    "        (\n",
    "            cos_similarity(test_left_vector_dict[id_left], test_right_vector_dict[id_right]),\n",
    "            utils.subdict(record_dict[id_left], field_list), utils.subdict(record_dict[id_right], field_list)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's still necessary to filter `found_pair_set` with a slower but more precise pairwise classifier.\n",
    "\n",
    "To do that, let's first save our model and data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.trainer.checkpoint_callback.best_model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use Google Drive to store the output files, please make sure to authorize Google Colab to use your Google Drive account once the cell below runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "rl_output_path = \"/content/drive/My Drive/trained-models/notebooks/rl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p \"{rl_output_path}/\"\n",
    "!cp {model.trainer.checkpoint_callback.best_model_path} \"{rl_output_path}/rl-model.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(f'{rl_output_path}/rl-train-records.json', 'w') as f:\n",
    "    json.dump(datamodule.train_record_dict, f, indent=4)\n",
    "\n",
    "with open(f'{rl_output_path}/rl-valid-records.json', 'w') as f:\n",
    "    json.dump(datamodule.valid_record_dict, f, indent=4)\n",
    "\n",
    "with open(f'{rl_output_path}/rl-test-records.json', 'w') as f:\n",
    "    json.dump(datamodule.test_record_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(f'{rl_output_path}/rl-train-pos-pairs.json', 'w') as f:\n",
    "    json.dump([list(pair) for pair in datamodule.train_pos_pair_set], f, indent=4)\n",
    "\n",
    "with open(f'{rl_output_path}/rl-valid-pos-pairs.json', 'w') as f:\n",
    "    json.dump([list(pair) for pair in datamodule.valid_pos_pair_set], f, indent=4)\n",
    "\n",
    "with open(f'{rl_output_path}/rl-test-pos-pairs.json', 'w') as f:\n",
    "    json.dump([list(pair) for pair in datamodule.test_pos_pair_set], f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's proceed with matching on [End-to-End-Matching-Example.ipynb](https://colab.research.google.com/github/vintasoftware/entity-embed/blob/google-collab-notebooks/notebooks/google-colab/End-to-End-Matching-Example.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
